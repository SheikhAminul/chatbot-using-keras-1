{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import preprocessing, utils, losses, layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import os\n",
    "import yaml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data & Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(dataDirectory):\n",
    "    fileList = os.listdir(dataDirectory + \"/\")\n",
    "    globals()[\"questions\"] = []\n",
    "    globals()[\"answers\"] = []\n",
    "    for file in fileList:\n",
    "        data = yaml.safe_load(open(dataDirectory + \"/\" + file, \"rb\"))\n",
    "        conversationList = data[\"conversations\"]\n",
    "        for conversation in conversationList:\n",
    "            for i in range(len(conversation) - 1):\n",
    "                questions.append(conversation[i])\n",
    "                answers.append(conversation[i +1])\n",
    "\n",
    "# load data from all files\n",
    "loadData(\"./English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenizeList(tokenizedList):\n",
    "    tokens = list(tokenizer.word_index.values())\n",
    "    words = list(tokenizer.word_index.keys())\n",
    "    return \" \".join([words[tokens.index(token)] for token in list(tokenizedList) if token in tokens])\n",
    "\n",
    "def tokenizeList(dataList):\n",
    "    tokenizedList = tokenizer.texts_to_sequences(dataList)\n",
    "    if(\"sentenceMaxlen\" not in globals()):\n",
    "        globals()[\"sentenceMaxlen\"] = max([len(x) for x in tokenizer.texts_to_sequences(questions + answers)])\n",
    "        globals()[\"vocabularyLen\"] = len(tokenizer.word_index) + 1\n",
    "    return preprocessing.sequence.pad_sequences(tokenizedList, maxlen=globals()[\"sentenceMaxlen\"], padding=\"post\")\n",
    "\n",
    "# convert answers and questions to token\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(questions + answers)\n",
    "tokenizedQuestions = tokenizeList(questions)\n",
    "tokenizedAnswers = tokenizeList(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Algorithms & Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheikh/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sheikh/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sheikh/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/home/sheikh/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sheikh/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.005353</td>\n",
       "      <td>0.006424</td>\n",
       "      <td>0.00571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.006136</td>\n",
       "      <td>0.009501</td>\n",
       "      <td>0.00670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Algorithm  Precision    Recall  F1 Score\n",
       "0  DecisionTreeClassifier   0.005353  0.006424   0.00571\n",
       "1      LogisticRegression   0.006136  0.009501   0.00670"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "\n",
    "models = [\n",
    "    DecisionTreeClassifier(random_state=0),\n",
    "    LogisticRegression(random_state=0)\n",
    "]\n",
    "\n",
    "# Precision, Recall and F1 Score\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#For Each Algorithm \n",
    "accuracyList = []\n",
    "for model in models:\n",
    "    modelName = model.__class__.__name__\n",
    "    #Split Data \n",
    "    X_train, X_test, y_train, y_test = train_test_split(tokenizedQuestions, answers, test_size=0.33, random_state=0)\n",
    "    #Train Algorithm\n",
    "    model.fit(X_train, y_train)\n",
    "    # Make Predictions\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracyList.append([modelName, precision_score(y_test, y_pred, average='macro'), recall_score(y_test, y_pred, average='macro'), f1_score(y_test, y_pred, average='macro')])\n",
    "\n",
    "pd.DataFrame(accuracyList, columns=[\"Algorithm\", \"Precision\", \"Recall\", \"F1 Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm doing well. How are you?\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "inp = \"How are you?\"\n",
    "pred = models[0].predict(tokenizeList([inp]))\n",
    "print(pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 200)         394800    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, None, 64)          51072     \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 43)                2795      \n",
      "=================================================================\n",
      "Total params: 456,923\n",
      "Trainable params: 456,923\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocabularyLen, output_dim=200))\n",
    "\n",
    "model.add(layers.GRU(64, return_sequences=True))\n",
    "\n",
    "model.add(layers.SimpleRNN(64))\n",
    "\n",
    "model.add(layers.Dense(sentenceMaxlen))\n",
    "\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 869 samples\n",
      "Epoch 1/15\n",
      "869/869 [==============================] - 4s 4ms/sample - loss: 22576.5846\n",
      "Epoch 2/15\n",
      "869/869 [==============================] - 1s 1ms/sample - loss: 30800.9060\n",
      "Epoch 3/15\n",
      "869/869 [==============================] - 1s 1ms/sample - loss: 27789.9088\n",
      "Epoch 4/15\n",
      "869/869 [==============================] - 1s 1ms/sample - loss: 26765.6243\n",
      "Epoch 5/15\n",
      "869/869 [==============================] - 1s 1ms/sample - loss: 17812.5051\n",
      "Epoch 6/15\n",
      "869/869 [==============================] - 1s 1ms/sample - loss: 13554.2425\n",
      "Epoch 7/15\n",
      "869/869 [==============================] - 1s 1ms/sample - loss: 13554.2426\n",
      "Epoch 8/15\n",
      "550/869 [=================>............] - ETA: 0s - loss: 13620.6996"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
    "model.fit(tokenizedQuestions, tokenizedAnswers, batch_size=50, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.23816282 -0.4505563   0.27331877 -0.3019432  -0.7532536  -0.252744\n",
      "  0.4943618   0.34916615  0.4996623  -0.31514668  0.4950322  -0.5292854\n",
      " -0.728397    0.2984823   0.09090549  0.145109    0.4390429   0.41034794\n",
      "  0.33624527 -0.2735399   0.2861031   0.42040667  0.3454618   0.04678136\n",
      " -0.8286958  -0.12820126 -0.24578056  0.39564025 -0.1883444  -0.37327543\n",
      "  0.0541236  -0.33886713  0.558437    0.02017151 -0.12680301 -0.1098565\n",
      " -0.12764136  0.20191672  0.02782319 -0.26283512  0.35495248 -0.13823554\n",
      "  0.08448514]\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "inp = \"Hi\"\n",
    "pred = model.predict(tokenizeList([inp]))\n",
    "print(pred[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
